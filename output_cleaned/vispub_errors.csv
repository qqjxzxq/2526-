conference,year,title,doi,abstract,authorNamesDeduped,award,accessible,early,resources,link,oa_title,oa_openalex_id,oa_doi,oa_cited_by_count,oa_publication_year,oa_referenced_works,oa_id_valid,oa_doi_valid,doi_valid,title_similarity,oa_referenced_works_parsed,ref_count,year_consistent,author_overlap,valid_score,valid_record
Vis,2025,An Intelligent Interactive Visual Analytics System for Exploring Large and Multi-Scale Pathology Images,EARLY_ACCESS/3ca02fd1-d387-44fa-b576-413ea269d949,"Pathology images are crucial for cancer diagnosis and treatment. Although artificial intelligence has driven rapid advancements in pathology image analysis, the interpretation of ultra-large and multi-scale pathology images in clinical practice still heavily relies on physicians' experience. Clinicians need to repeatedly zoom in and out on individual slides to compare and assess pathological details — a process that is both time-consuming and prone to visual fatigue. The system first employs a diffusion model to perform tissue segmentation on pathology images, then calculates pathological tissue proportions and morphological metrics. Finally, through multi-scale dynamic comparison and multi-level visual evaluation, the system facilitates comprehensive and precise analysis of pathology images. The system provides clinicians with an intelligent and interactive tool for pathology image interpretation, enabling efficient visualization and precise analysis of pathological details, thereby reducing the effort require for detailed analysis.",Chaoqing Xu;Ruiqi Yang;Weihan Li;Xinyuan Fu;Liting Fang;Zunlei Feng;Can Wang;Mingli Song;Wei Chen,,,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2F3ca02fd1-d387-44fa-b576-413ea269d949,An Intelligent Interactive Visual Analytics System for Exploring Large and Multi-Scale Pathology Images,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation,EARLY_ACCESS/e8f720e9-fa87-43c1-9d57-b8cc150f3cd3,"Biomedical data harmonization is essential for enabling exploratory analyses and meta-studies, but the process of schema matching—identifying semantic correspondences between elements of disparate datasets (schemas)—remains a labor-intensive and error-prone task. Even state-of-the-art automated methods often yield low accuracy when applied to biomedical schemas due to the large number of attributes and nuanced semantic differences between them. We present BDIViz, a novel visual analytics system designed to streamline the schema matching process for biomedical data. Through formative studies with domain experts, we identified key requirements for an effective solution and developed interactive visualization techniques that address both scalability challenges and semantic ambiguity. BDIViz employs an ensemble approach that combines multiple matching methods with LLM-based validation, summarizes matches through interactive heatmaps, and provides coordinated views that enable users to quickly compare attributes and their values. Our method-agnostic design allows the system to integrate various schema matching algorithms and adapt to application-specific needs. Through two biomedical case studies and a within-subject user study with  domain experts, we demonstrate that BDIViz significantly improves matching accuracy while reducing cognitive load and curation time compared to baseline approaches.",Eden Wu;Dishita Turakhia;Guande Wu;Christos Koutras;Sarah Keegan;Wenke Liu;Beata Szeitz;David Fenyo;Claudio Silva;Juliana Freire,,True,True,P;C;O,https://vispubs.com/?paper=EARLY_ACCESS%2Fe8f720e9-fa87-43c1-9d57-b8cc150f3cd3,BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Causality-based Visual Analytics of Sentiment Contagion in Social Media Topics,EARLY_ACCESS/bf2f93ab-ab67-4a81-8726-6819e84553be,"Sentiment contagion occurs when attitudes toward one topic are influenced by attitudes toward others.
Detecting and understanding this phenomenon is essential for analyzing topic evolution and informing social policies.
Prior research has developed models to simulate the contagion process through hypothesis testing and has visualized user–topic correlations to aid comprehension.
Nevertheless, the vast volume of topics and the complex interrelationships on social media present two key challenges: (1) efficient construction of large-scale sentiment contagion networks, and (2) in-depth explorations of these networks.
To address these challenges, we introduce a causality-based framework that efficiently constructs and explains sentiment contagion.
We further propose a map-like visualization technique that encodes time using a horizontal axis, enabling efficient visualization of causality-based sentiment flow while maintaining scalability through limitless spatial segmentation.
Based on the visualization, we develop CausalMap, a system that supports analysts in tracing sentiment contagion pathways and assessing the influence of different demographic groups.
Furthermore, we conduct comprehensive evaluations——including two use cases, a task-based user study, an expert interview, and an algorithm evaluation——to validate the usability and effectiveness of our approach.",Renzhong Li;Shuainan Ye;Yuchen Lin;Buwei Zhou;Zhining Kang;Tai-Quan Peng;Wenhao Fu;Tan Tang;Yingcai Wu,BP,,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2Fbf2f93ab-ab67-4a81-8726-6819e84553be,Causality-based Visual Analytics of Sentiment Contagion in Social Media Topics,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,CD-TVD:Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data,EARLY_ACCESS/4161480d-1e23-4eed-be76-3953fac207b7,"Large-scale scientific simulations require significant resources to generate high-resolution time-varying data (TVD). While super-resolution is an efficient post-processing strategy to reduce costs, existing methods rely on a large amount of HR training data, limiting their applicability to diverse simulation scenarios. To address this constraint, we proposed CD-TVD, a novel framework that combines contrastive learning and an improved diffusion-based super-resolution model to achieve accurate 3D super-resolution from limited time-step high-resolution data. During pre-training on historical simulation data, the contrastive encoder and diffusion super-resolution modules learn degradation patterns and detailed features of high-resolution and low-resolution samples. In the training phase, the improved diffusion model with a local attention mechanism is fine-tuned using only one newly generated high-resolution timestep, leveraging the degradation knowledge learned by the encoder. This design minimizes the reliance on large-scale high-resolution datasets while maintaining the capability to recover fine-grained details. Experimental results on fluid and atmospheric simulation datasets confirm that CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a significant advancement in data augmentation for large-scale scientific simulations. The code is available at https://github.com/Xin-Gao-private/CD-TVD.",Chongke Bi;Xin Gao;Jiakang Deng;Guan Li;Jun Han,HM,True,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F4161480d-1e23-4eed-be76-3953fac207b7,CD-TVD:Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies,EARLY_ACCESS/da138859-6a6c-4df2-813b-6a70e20d04a1,"In-depth analysis of competitive debates is essential for participants to develop argumentative skills and refine strategies, and further improve their debating performance. However, manual analysis of unstructured and unlabeled textual records of debating is time-consuming and ineffective, as it is challenging to reconstruct contextual semantics and track logical connections from raw data. To address this, we propose Conch, an interactive visualization system that systematically analyzes both what is debated and how it is debated. In particular, we propose a novel parallel spiral visualization that compactly traces the multidimensional evolution of clash points and participant interactions throughout debate process. In addition, we leverage large language models with well-designed prompts to automatically identify critical debate elements such as clash points, disagreements, viewpoints, and strategies, enabling participants to understand the debate context comprehensively. Finally, through two case studies on real-world debates and a carefully-designed user study, we demonstrate Conch's effectiveness and usability for competitive debate analysis.",Qianhe Chen;Yong WANG;Yixin Yu;Xiyuan Zhu;Xuerou Yu;Ran Wang,,True,True,PW;P;O,https://vispubs.com/?paper=EARLY_ACCESS%2Fda138859-6a6c-4df2-813b-6a70e20d04a1,Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,"Data Speaks, But Who Gives It a Voice? Understanding Persuasive Strategies in Data-Driven News Articles",EARLY_ACCESS/9e9a15b7-6ccb-4419-b391-e82e3bd8b907,"Data-driven news articles combine narrative storytelling with data visualizations to inform and influence public opinion on pressing societal issues. These articles often employ persuasive strategies, which are rhetorical techniques in narrative framing, visual rhetoric, or data presentation, to influence audience interpretation and opinion formation regarding information communication. While previous research has examined whether and when data visualizations persuade, the strategic choices made by persuaders remain largely unexplored. Addressing this gap, our work presents a taxonomy of persuasive strategies grounded in psychological theories and expert insights, categorizing 15 strategies across five dimensions: Credibility, Guided Interpretation, Reference-based Framing, Emotional Appeal, and Participation Invitation. To facilitate large-scale analysis, we curated a dataset of 936 data-driven news articles annotated with both persuasive strategies and their perceived effects. Leveraging this corpus, we developed a multimodal, multi-task learning model that jointly predicts the presence of persuasive strategies and their persuasive effects by incorporating both embedded (text and visualization) and explicit (visual narrative and psycholinguistic) features. Our evaluation demonstrates that our model outperforms state-of-the-art baselines in identifying persuasive strategies and measuring their effects.",Zikai Li;Chuyi Zheng;Ziang Li;Yang Shi,,,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2F9e9a15b7-6ccb-4419-b391-e82e3bd8b907,"Data Speaks, But Who Gives It a Voice? Understanding Persuasive Strategies in Data-Driven News Articles",,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning,EARLY_ACCESS/f4073f80-c0bd-4d83-9c72-3dcfd060ef7f,"Although data visualization is powerful for revealing patterns and communicating insights, creating effective visualizations requires familiarity with authoring tools and often disrupts the analysis flow. While large language models show promise for automatically converting analysis intent into visualizations, existing methods function as black boxes without transparent reasoning processes, which prevents users from understanding design rationales and refining suboptimal outputs. To bridge this gap, we propose integrating Chain-of-Thought (CoT) reasoning into the Natural Language to Visualization (NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for NL2VIS and develop an automatic pipeline to equip existing datasets with structured reasoning steps. Second, we introduce nvBench-CoT, a specialized dataset capturing detailed step-by-step reasoning from ambiguous natural language descriptions to finalized visualizations, which enables state-of-the-art performance when used for model fine-tuning. Third, we develop DeepVIS, an interactive visual interface that tightly integrates with the CoT reasoning process, allowing users to inspect reasoning steps, identify errors, and make targeted adjustments to improve visualization outcomes. Quantitative benchmark evaluations, two use cases, and a user study collectively demonstrate that our CoT framework effectively enhances NL2VIS quality while providing insightful reasoning steps to users.",Zhihao Shuai;Boyan LI;siyu yan;Yuyu Luo;Weikai Yang,,,True,P;C;O,https://vispubs.com/?paper=EARLY_ACCESS%2Ff4073f80-c0bd-4d83-9c72-3dcfd060ef7f,DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,DKMap: Interactive Exploration of Vision-Language Alignment in Multimodal Embeddings via Dynamic Kernel Enhanced Projection,EARLY_ACCESS/3d649242-7596-4300-84fe-793b6a69b69c,"Examining vision-language alignment in multimodal embeddings is crucial for various tasks, such as evaluating generative models and filtering pretraining data. The intricate nature of high-dimensional features necessitates dimensionality reduction (DR) methods to explore alignment of multimodal embeddings. However, existing DR methods fail to account for cross-modal alignment metrics, resulting in severe occlusion of points with divergent metrics clustered together, inaccurate contour maps from over-aggregation, and insufficient support for multi-scale exploration. To address these problems, this paper introduces DKMap, a novel DR visualization technique for interactive exploration of multimodal embeddings through Dynamic Kernel enhanced projection. First, rather than performing dimensionality reduction and contour estimation sequentially, we introduce a kernel regression supervised t-SNE that directly integrates post-projection contour mapping into the projection learning process, ensuring cross-modal alignment mapping accuracy. Second, to enable multi-scale exploration with dynamic zooming and progressively enhanced local detail, we integrate validation-constrained α refinement of a generalized t-kernel with quad-tree-based multi-resolution technique, ensuring reliable kernel parameter tuning without overfitting. DKMap is implemented as a multi-platform visualization tool, featuring a web-based system for interactive exploration and a Python package for computational notebook analysis. Quantitative comparisons with baseline DR techniques demonstrate DKMap’s superiority in accurately mapping cross-modal alignment metrics. We further demonstrate generalizability and scalability of DKMap with three usage scenarios, including visualizing million-scale text-to-image corpus, comparatively evaluating generative models, and exploring a billion-scale pretraining dataset.",Yilin Ye;Chenxi Ruan;Yu Zhang;Zikun Deng;Wei Zeng,,,True,C;O,https://vispubs.com/?paper=EARLY_ACCESS%2F3d649242-7596-4300-84fe-793b6a69b69c,DKMap: Interactive Exploration of Vision-Language Alignment in Multimodal Embeddings via Dynamic Kernel Enhanced Projection,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,EmbryoProfiler: A Visual Clinical Decision Support System for IVF,EARLY_ACCESS/28a70097-2b4f-4ac5-9bb0-7b452093c16b,"In-vitro fertilization (IVF) has become standard practice to address infertility, which affects more than one in ten couples in the US. However, current protocols yield relatively low success rates of about 20% per treatment cycle. A critical but complex and time-consuming step is the grading and selection of embryos for implantation. Although incubators with time-lapse microscopy have enabled computational analysis of embryo development, existing automated approaches either require extensive manual annotations or use opaque deep learning models that are hard for clinicians to validate and trust. We present EmbryoProfiler, a visual analytics system collaboratively developed with embryologists, biologists, and machine learning researchers to support clinicians in visually assessing embryo viability from time-lapse microscopy imagery. Our system incorporates a deep learning pipeline that automatically annotates microscopy images and extracts clinically interpretable features relevant for embryo grading. Our contributions include: (1) a semi-automatic, visualization-based workflow that guides clinicians through fertilization assessment, developmental timing evaluation, morphological inspection, and comparative analysis of embryos; (2) innovative interactive visualizations, such as cell-shape plots, designed to facilitate efficient analysis of morphological and developmental characteristics; and (3) an integrated, explainable machine learning classifier offering transparent, clinically-informed embryo viability scoring to predict live birth outcomes. Quantitative evaluation of our classifier and qualitative case studies conducted with practitioners demonstrate that EmbryoProfiler enables clinicians to make better-informed embryo selection decisions, potentially leading to improved clinical outcomes in IVF treatments.",Johannes Knittel;Simon Warchol;Jakob Troidl;Camelia D. Brumar;Helen Yang;Eric Mörth;Robert Krüger;Daniel Needleman;Dalit Ben-Yosef;Hanspeter Pfister,,,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2F28a70097-2b4f-4ac5-9bb0-7b452093c16b,EmbryoProfiler: A Visual Clinical Decision Support System for IVF,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Evaluating judgements of spatial correlation in visual displays of scalar field distributions,EARLY_ACCESS/14e8969b-dbaf-41b3-8330-a0decc6b53b0,"In this work we study the identification of spatial correlation in distributions of 2D scalar fields, presented across different forms of visual displays. We study simple visual displays that directly show color-mapped scalar fields, namely those drawn from a distribution, and whether humans can identify strongly correlated spatial regions in these displays. In this setting, the recognition of correlation requires making judgments on a set of fields, rather than just one field. Thus, in our experimental design we compare two basic visualization designs: animation-based displays against juxtaposed views of scalar fields, along different choices of color scales. Moreover, we investigate the impacts of the distribution itself, controlling for the level of spatial correlation and discriminability in spatial
scales. Our study’s results illustrate the impacts of these distribution characteristics, while also highlighting how different visual displays impact the types of judgments made in assessing spatial correlation. Supplemental material is available at https://osf.io/zn4qy/",Yayan Zhao;Matthew Berger,,,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F14e8969b-dbaf-41b3-8330-a0decc6b53b0,Evaluating judgements of spatial correlation in visual displays of scalar field distributions,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Exploring 3D Unsteady Flow using 6D Observer Space Interactions,EARLY_ACCESS/fb22d5c6-8a2f-4d98-80ec-1fbbfed18538,"Visualizing and analyzing 3D unsteady flow fields is a very challenging task.
We approach this problem by leveraging the mathematical foundations of 3D observer fields to explore and analyze 3D flows in reference frames that are more suitable to visual analysis than the input reference frame.
We design novel interactive tools for determining, filtering, and combining reference frames for observer-aware 3D unsteady flow visualization. We represent the space of reference frame motions in a 3D spatial domain via a 6D parameter space, in which every observer is a time-dependent curve. Our framework supports operations in this 6D observer space by separately focusing on two 3D subspaces, for 3D translations, and 3D rotations, respectively. We show that this approach facilitates a variety of interactions with 3D flow fields.
Building on the interactive selection of observers, we furthermore introduce novel techniques such as observer-aware streamline- and pathline-filtering as well as observer-aware isosurface animations of scalar fluid properties for the enhanced visualization and analysis of 3D unsteady flows.
We discuss the theoretical underpinnings as well as practical implementation considerations of our approach, and demonstrate the benefits of its 6+1D observer-based methodology on several 3D unsteady flow datasets.",Xingdi Zhang;Amani Ageeli;Thomas Theußl;Markus Hadwiger;Peter Rautek,HM,True,True,P;C;O,https://vispubs.com/?paper=EARLY_ACCESS%2Ffb22d5c6-8a2f-4d98-80ec-1fbbfed18538,Exploring 3D Unsteady Flow using 6D Observer Space Interactions,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Eye of the Beholder: Towards Measuring Visualization Complexity,EARLY_ACCESS/a610d354-3cae-4df3-ac69-db5a5a7572a6,"Constructing expressive and legible visualizations is a key activity for visualization designers.While numerous design guidelines exist, research on how specific graphical features affect perceived visual complexity remains limited. In this paper, we report on a crowdsourced study to collect human ratings of perceived complexity for diverse visualizations. Using these ratings as ground truth, we then evaluated three methods to estimate this perceived complexity: image analysis metrics, multilinear regression using manually coded visualization features, and automated feature extraction using a large language model (LLM). Image complexity metrics showed no correlation with human-perceived visualization complexity. Manual feature coding produced a reasonable predictive model but required substantial effort. In contrast, a zero-shot LLM (GPT-4o mini) demonstrated strong capabilities in both rating complexity and extracting relevant features. Our findings suggest that visualization complexity is truly in the eye of the beholder, yet can be effectively approximated using zero-shot LLM prompting, offering a scalable approach for evaluating the complexity of visualizations. The dataset and code for the study and data analysis can be found at https://osf.io/w85a4/.",Johannes Ellemose;Niklas Elmqvist,,True,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2Fa610d354-3cae-4df3-ac69-db5a5a7572a6,Eye of the Beholder: Towards Measuring Visualization Complexity,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,F^2Stories: A Modular Framework for Multi-Objective Optimization of Storylines with a Focus on Fairness,EARLY_ACCESS/bbfc6b4e-96d6-4981-87da-8bb285d256dc,"Storyline visualizations represent character interactions over time. When these characters belong to different groups, a new research question emerges: how can we balance optimization of readability across the groups while preserving the overall narrative structure of the story? Traditional algorithms that optimize global readability metrics (like minimizing crossings) can introduce quality biases between the different groups based on their cardinality and other aspects of the data. Visual consequences of these biases are: making characters of minority groups disproportionately harder to follow, and visually deprioritizing important characters when their curves become entangled with numerous secondary characters. We present F2Stories, a modular framework that addresses these challenges in storylines by offering three complementary optimization modes: (1) fairnessMode ensures that no group bears a disproportionate burden of visualization complexity regardless of their representation in the story; (2) focusMode allows prioritizing a group of characters while maintaining good readability for secondary characters; and (3) standardMode globally optimizes classical aesthetic metrics. Our approach is based on Mixed Integer Linear Programming (MILP), offering optimality guarantees, precise balancing of competing metrics through weighted objectives, and the flexibility to incorporate complex fairness concepts as additional constraints without the need to redesign the entire algorithm. We conducted an extensive experimental analysis to demonstrate how F2Stories enables more fair or focus group-prioritized storyline visualizations while maintaining adherence to established layout constraints. Our evaluation includes comprehensive results from a detailed case study that shows the effectiveness of our approach in real-world narrative contexts. An open access copy of this paper and all supplemental materials are available at https://osf.io/e2qvy/.",Tommaso Piselli;Giuseppe Liotta;Fabrizio Montecchiani;Martin Nöllenburg;Sara Di Bartolomeo,HM,,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2Fbbfc6b4e-96d6-4981-87da-8bb285d256dc,F^2Stories: A Modular Framework for Multi-Objective Optimization of Storylines with a Focus on Fairness,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,FlowForge: Guiding the Creation of Multi-agent Workflows with Interactive Visualizations as a Thinking Scaffold,EARLY_ACCESS/ed3195e2-8726-4d85-acb7-c5ed2dc361bb,"Multi-agent workflows have become a powerful approach to solve complicated tasks by decomposing them into multiple sub-tasks and assigning the sub-tasks to specialized agents. However, designing optimal workflows remains challenging due to the expansive design space. Current practices rely heavily on practitioner intuition and expertise, often resulting in design fixation or an unstructured trial-and-error exploration. To address these challenges, this work introduces FlowForge, an interactive visualization tool to facilitate multi-agent workflow creation through i) a structured visual explore of the design space and ii) in-situ guidance based on design patterns. Based on formative studies and literature review, FlowForge organizes the workflow design process into three levels (i.e., task planning, agent assignment, and agent optimization), ranging from abstract to concrete. A structured visual exploration of the design space enable users to transition from high-level concepts to detailed implementations, and to compare alternative solutions across multiple performance metrics. Additionally, drawing from established workflow design patterns, FlowForge provides contextually relevant in-situ suggestions at each level as users navigate the design space. Use cases and expert interviews demonstrate the usability and effectiveness of FlowForge, while also yielding valuable observations into how practitioners explore the design space and leverage guidance in workflow creation.",Pan Hao;Dongyeop Kang;Nicholas Hinds;Qianwen Wang,,,True,PW;P;C;O,https://vispubs.com/?paper=EARLY_ACCESS%2Fed3195e2-8726-4d85-acb7-c5ed2dc361bb,FlowForge: Guiding the Creation of Multi-agent Workflows with Interactive Visualizations as a Thinking Scaffold,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,From Vision to Touch: Bridging Visual and Tactile Principles for Accessible Data Representation,EARLY_ACCESS/544233de-86be-4bc4-ab7e-2e5de7d366b2,"Tactile graphics are widely used to present maps and statistical diagrams to blind and low vision (BLV) people, with accessibility guidelines recommending their use for graphics where spatial relationships are important. Their use is expected to grow with the advent of commodity refreshable tactile displays. However, in stark contrast to visual information graphics, we lack a clear understanding of the benefits that well-designed tactile information graphics offer over text descriptions for BLV people. To address this gap, we introduce a framework considering the three components of encoding, perception and cognition to examine the known benefits for visual information graphics and explore their applicability to tactile information graphics. This work establishes a preliminary theoretical foundation for the tactile-first design of information graphics and identifies future research avenues.",Kim Marriott;Matthew Butler;Leona Holloway;William Jolley;Bongshin Lee;Bruce Maguire;Danielle Szafir,,True,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2F544233de-86be-4bc4-ab7e-2e5de7d366b2,From Vision to Touch: Bridging Visual and Tactile Principles for Accessible Data Representation,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis,EARLY_ACCESS/5cec6278-7356-4658-8125-0dc6b65f8b08,"Unstructured meshes present challenges in scientific data analysis due to irregular distribution and complex connectivity. Computing and storing connectivity information is a major bottleneck for visualization algorithms, affecting both time and memory performance. Recent task-parallel data structures address this by precomputing connectivity information at runtime while the analysis algorithm executes, effectively hiding computation costs and improving performance. However, existing approaches are CPU-bound, forcing the data structure and analysis algorithm to compete for the same computational resources, limiting potential speedups. To overcome this limitation, we introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU systems. Specifically, we offload the computation of mesh connectivity information to GPU threads, enabling CPU threads to focus on executing the visualization algorithm. Following this paradigm, we propose GPU-Aided Localized data structurE (GALE), the first open-source CUDA-based data structure designed for heterogeneous task parallelism. Experiments on two 20-core CPUs and an NVIDIA V100 GPU show that GALE achieves up to 2.7× speedup over state-of-the-art localized data structures while maintaining memory efficiency.",Guoxi Liu;Thomas Randall;Rong Ge;Federico Iuricich,,,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F5cec6278-7356-4658-8125-0dc6b65f8b08,GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,"GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP",EARLY_ACCESS/28a61ed9-3f15-4835-a688-e72b1fd6fa0c,"Despite the widespread use of Uniform Manifold Approximation and Projection (UMAP), the impact of its stochastic optimization process on the results remains underexplored. We observed that it often produces unstable results where the projections of data points are determined mostly by chance rather than reflecting neighboring structures. To address this limitation, we introduce (r,d)-stability to UMAP: a framework that analyzes the stochastic positioning of data points in the projection space. To assess how stochastic elements—specifically, initial projection positions and negative sampling—impact UMAP results, we introduce “ghosts”, or duplicates of data points representing potential positional variations due to stochasticity. We define a data point’s projection as (r,d)-stable if its ghosts perturbed within a circle of radius r in the initial projection remain confined within a circle of radius d for their final positions. To efficiently compute the ghost projections, we develop an adaptive dropping scheme that reduces a runtime up to 60% compared to an unoptimized baseline while maintaining approximately 90% of unstable points. We also present a visualization tool that supports the interactive exploration of the (r,d)-stability of data points. Finally, we demonstrate the effectiveness of our framework by examining the stability of projections of real-world datasets and present usage guidelines for the effective use of our framework.",Myeongwon Jung;Takanori Fujiwara;Jaemin Jo,,True,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F28a61ed9-3f15-4835-a688-e72b1fd6fa0c,"GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP",,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,GoFish: A Grammar of More Graphics!,EARLY_ACCESS/12e7281f-5e70-4d16-a9a0-a87a0d46df5c,"Visualization grammars from ggplot2 to Vega-Lite are based on the Grammar of Graphics (GoG), our most comprehensive formal theory of visualization. The GoG helped expand the expressive gamut of visualization by moving beyond fixed chart types and towards a design space of composable operators. Yet, the resultant design space has surprising limitations, inconsistencies, and cliffs---even seemingly simple charts like mosaics, waffles, and ribbons fall out of scope of most GoG implementations. To author such charts, visualization designers must either rely on overburdened grammar developers to implement purpose-built mark types (thus reintroducing the issues of typologies) or drop to lower-level frameworks. In response, we present GoFish: a declarative visualization grammar that formalizes Gestalt principles (e.g., uniform spacing, containment, and connection) that have heretofore been complected in GoG constructs. These graphical operators achieve greater expressive power than their predecessors by enabling recursive composition: they can be nested and overlapped arbitrarily. Through a diverse example gallery, we demonstrate how graphical operators free users to arrange shapes in many different ways while retaining the benefits of high-level grammars like scale resolution and coordinate transform management. Recursive composition naturally yields an infinite design space that blurs the boundary between an expressive, low-level grammar and a concise, high-level one. In doing so, we point towards an updated theory of visualization, one that is open to an innumerable space of graphic representations instead of limited to a fixed set of good designs.",Josh Pollock;Arvind Satyanarayan,,True,True,PW;O,https://vispubs.com/?paper=EARLY_ACCESS%2F12e7281f-5e70-4d16-a9a0-a87a0d46df5c,GoFish: A Grammar of More Graphics!,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Here's what you need to know about my data: Exploring Expert Knowledge's Role in Data Analysis,EARLY_ACCESS/849ca631-398a-4087-9f66-a0837fc3af86,"Data-driven decision making has become a popular practice in science, industry, and public policy. Yet data alone, as an imperfect and partial representation of reality, is often insufficient to make good analysis decisions. Knowledge about the context of a dataset, its strengths and weaknesses, and its applicability for certain tasks is essential. Analysts are often not only familiar with the data itself, but also have data hunches about their analysis subject. In this work, we present an interview study with analysts from a wide range of domains and with varied expertise and experience, inquiring about the role of contextual knowledge. We provide insights into how data is insufficient in analysts' workflows and how they incorporate other sources of knowledge into their analysis. We analyzed how knowledge of data shaped their analysis outcome. Based on the results, we suggest design opportunities to better and more robustly consider both knowledge and data in analysis processes.",Haihan Lin;Maxim Lisnic;Derya Akbaba;Miriah Meyer;Alexander Lex,HM,True,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F849ca631-398a-4087-9f66-a0837fc3af86,Here's what you need to know about my data: Exploring Expert Knowledge's Role in Data Analysis,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Locally Adapted Reference Frame Fields using Moving Least Squares,EARLY_ACCESS/b71e9ec2-8579-410a-94c0-a9e2812a2712,"The detection and analysis of features in fluid flow are important tasks in fluid mechanics and flow visualization. One recent class of methods to approach this problem is to first compute objective optimal reference frames, relative to which the input vector field becomes as steady as possible. However, existing methods either optimize locally over a fixed neighborhood, which might not match the extent of interesting features well, or perform global optimization, which is costly. We propose a novel objective method for the computation of optimal reference frames that automatically adapts to the flow field locally, without having to choose neighborhoods a priori. We enable adaptivity by formulating this problem as a moving least squares approximation, through which we determine a continuous field of reference frames. To incorporate fluid features into the computation of the reference frame field, we introduce the use of a scalar guidance field into the moving least squares approximation. The guidance field determines a curved manifold on which a regularly sampled input vector field becomes a set of irregularly spaced samples, which then forms the input to the moving least squares approximation. Although the guidance field can be any scalar field, by using a field that corresponds to flow features the resulting reference frame field will adapt accordingly. We show that using an FTLE field as the guidance field results in a reference frame field that adapts better to local features in the flow than prior work. However, our moving least squares framework is formulated in a very general way, and therefore other types of guidance fields could be used in the future to adapt to local fluid features.",Julio Rey Ramirez;Peter Rautek;Tobias Günther;Markus Hadwiger,,,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2Fb71e9ec2-8579-410a-94c0-a9e2812a2712,Locally Adapted Reference Frame Fields using Moving Least Squares,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,"MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models",EARLY_ACCESS/e7cc7c10-4310-4753-b586-467106cdc883,"Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models (LLMs) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming LLM-based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data communication.",Amit Kumar Das;Klaus Mueller,,,True,P;C;O,https://vispubs.com/?paper=EARLY_ACCESS%2Fe7cc7c10-4310-4753-b586-467106cdc883,"MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models",,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Mixture of Cluster-guided Experts for Retrieval-Augmented Label Placement,EARLY_ACCESS/4becbe1b-2bde-4b12-adfd-e45a36678884,"Text labels are widely used to convey auxiliary information in visualization and graphic design. The substantial variability in the categories and structures of labeled objects leads to diverse label layouts. Recent single-model learning-based solutions in label placement struggle to capture fine-grained differences between these layouts, which in turn limits their performance. In addition, although human designers often consult previous works to gain design insights, existing label layouts typically serve merely as training data, limiting the extent to which embedded design knowledge can be exploited. To address these challenges, we propose a mixture of cluster-guided experts (MoCE) solution for label placement. In this design, multiple experts jointly refine layout features, with each expert responsible for a specific cluster of layouts. A cluster-based gating function assigns input samples to experts based on representation clustering. We implement this idea through the Label Placement Cluster-guided Experts (LPCE) model, in which a MoCE layer integrates multiple feed-forward networks (FFNs), with each expert composed of a pair of FFNs. Furthermore, we introduce a retrieval augmentation strategy into LPCE, which retrieves and encodes reference layouts for each input sample to enrich its representations. Extensive experiments demonstrate that LPCE achieves superior performance in label placement, both quantitatively and qualitatively, surpassing a range of state-of-the-art baselines. Our algorithm is available at https://github.com/PingshunZhang/LPCE.",Pingshun Zhang;Enyu Che;Yinan Chen;Bingyao Huang;Haibin Ling;Jingwei Qu,,True,True,PW;O,https://vispubs.com/?paper=EARLY_ACCESS%2F4becbe1b-2bde-4b12-adfd-e45a36678884,Mixture of Cluster-guided Experts for Retrieval-Augmented Label Placement,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Neighbourhood-Preserving Voronoi Treemaps,EARLY_ACCESS/093d9538-804e-4951-a5bf-dab72e99c35d,"Voronoi treemaps are used to depict nodes and their hierarchical relationships simultaneously. However, in addition to the hierarchical structure, data attributes, such as co-occurring features or similarities, frequently exist. Examples include geographical attributes like shared borders between countries or contextualized semantic information such as embedding vectors derived from large language models. In this work, we introduce a Voronoi treemap algorithm that leverages data similarity to generate neighborhood-preserving treemaps. First, we extend the treemap layout pipeline to consider similarity during data preprocessing. We then use a Kuhn-Munkres matching of similarities to centroidal Voronoi tessellation (CVT) cells to create initial Voronoi diagrams with equal cell sizes for each level. Greedy swapping is used to improve the neighborhoods of cells to match the data's similarity further. During optimization, cell areas are iteratively adjusted to their respective sizes while preserving the existing neighborhoods. We demonstrate the practicality of our approach through multiple real-world examples drawn from infographics and linguistics. To quantitatively assess the resulting treemaps, we employ treemap metrics and measure neighborhood preservation.",Patrick Paetzold;Rebecca Kehlbeck;Yumeng Xue;Bin Chen;Yunhai Wang;Oliver Deussen,,,True,PW;P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F093d9538-804e-4951-a5bf-dab72e99c35d,Neighbourhood-Preserving Voronoi Treemaps,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,PiCCL: Data-Driven Mark Composition of Bespoke Pictorial Chart,EARLY_ACCESS/1513daf8-e0da-48f1-b800-f7b0975869ff,"We present PiCCL (Pictorial Chart Composition Language), a new language that enables users to easily create pictorial charts using a set of simple operators. To support systematic construction while addressing the main challenge of expressive pictorial chart authoring–manual composition and fine-tuning of visual properties–PiCCL introduces a parametric representation that integrates data-driven chart generation with graphical composition. It also employs a lazy data-binding mechanism that automatically synthesizes charts. PiCCL is grounded in a comprehensive analysis of real-world pictorial chart examples. We describe PiCCL’s design and its implementation as piccl.js, a JavaScript-based library. To evaluate PiCCL, we showcase a gallery that demonstrates its expressiveness and report findings from a user study assessing the usability of piccl.js. We conclude with a discussion of PiCCL’s limitations and potential, as well as future research directions.",Haoyan Shi;Yunhai Wang;Junhao Chen;Chenglong Wang;Bongshin Lee,,,True,PW;O,https://vispubs.com/?paper=EARLY_ACCESS%2F1513daf8-e0da-48f1-b800-f7b0975869ff,PiCCL: Data-Driven Mark Composition of Bespoke Pictorial Chart,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,"Probing the Visualization Literacy of Vision Language Models: the Good, the Bad, and the Ugly",EARLY_ACCESS/6f935187-a32b-477d-90a0-1a7e959c6222,"Vision Language Models (VLMs) demonstrate promising chart comprehension capabilities. Yet, prior explorations of their visualization literacy have been limited to assessing their response correctness and fail to explore their internal reasoning. To address this gap, we adapted attention-guided class activation maps (AG-CAM) for VLMs to visualize the influence and importance of input features (image and text) on model responses.  Using this approach,  we conducted an examination of four open-source (ChartGemma, Janus 1B and 7B, and LLaVA) and two closed-source (GPT-4o, Gemini) models comparing their performance and, for the open-source models, their AG-CAM results. Overall, we found that ChartGemma, a 3B parameter VLM fine-tuned for chart question-answering (QA), outperformed other open-source models and exhibited performance on par with significantly larger closed-source VLMs. We also found that VLMs exhibit spatial reasoning by accurately localizing key chart features, and semantic reasoning by associating visual elements with corresponding data values and query tokens. Our approach is the first to demonstrate the use of AG-CAM on early fusion VLM architectures, which are widely used, and for chart QA. We also show preliminary evidence that these results can align with human reasoning. Our promising open-source VLMs results pave the way for transparent and reproducible research in AI visualization literacy.",Lianghan Dong;Anamaria Crisan,,,True,PW;P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F6f935187-a32b-477d-90a0-1a7e959c6222,"Probing the Visualization Literacy of Vision Language Models: the Good, the Bad, and the Ugly",,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,"Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles",EARLY_ACCESS/29656e00-00d7-4080-b589-2d7c838c72be,"Design studies aim to develop visualization solutions for real-world problems across various application domains. Recently, the emergence of large language models (LLMs) has introduced new opportunities to enhance the design study process, providing capabilities such as creative problem-solving, data handling, and insightful analysis. However, despite their growing popularity, there remains a lack of systematic understanding of how LLMs can effectively assist researchers in visualization-specific design studies. In this paper, we conducted a multi-stage qualitative study to fill this gap, which involved 30 design study researchers from diverse backgrounds and expertise levels. Through in-depth interviews and carefully-designed questionnaires, we investigated strategies for utilizing LLMs, the challenges encountered, and the practices used to overcome them. We further compiled the roles that LLMs can play across different stages of the design study process. Our findings highlight practical implications to inform visualization practitioners, and also provide a framework for leveraging LLMs to facilitate the design study process in visualization research.",Shaolun Ruan;Rui Sheng;Xiaolin Wen;Jiachen Wang;Tianyi Zhang;Yong WANG;Tim Dwyer;Jiannan Li,HM,True,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F29656e00-00d7-4080-b589-2d7c838c72be,"Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles",,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,ReVISit 2: A Full Experiment Life Cycle User Study Framework,EARLY_ACCESS/ebdd5b65-b5cb-4d4b-bd78-92ff8cc2f243,"Online user studies of visualizations, visual encodings, and interaction techniques are ubiquitous in visualization research. Yet, designing, conducting, and analyzing studies effectively is still a major burden. Although various packages support such user studies, most solutions address only facets of the experiment life cycle, make reproducibility difficult, or do not cater to nuanced study designs or interactions. We introduce reVISit 2, a software framework that supports visualization researchers at all stages of designing and conducting browser-based user studies. ReVISit supports researchers in the design, debug & pilot, data collection, analysis, and dissemination experiment phases by providing both technical affordances (such as replay of participant interactions) and sociotechnical aids (such as a mindfully maintained community of support). It is a proven system that can be (and has been) used in publication-quality studies — which we demonstrate through a series of experimental replications. We reflect on the design of the system via interviews and an analysis of its technical dimensions. Through this work, we seek to elevate the ease with which studies are conducted, improve the reproducibility of studies within our community, and support the construction of advanced interactive studies.",Zach Cutler;Jack Wilburn;Hilson Shrestha;Yiren Ding;Brian Bollen;Khandaker Abrar Nadib;Tingying He;Andrew McNutt;Lane Harrison;Alexander Lex,BP,True,True,PW;P;O,https://vispubs.com/?paper=EARLY_ACCESS%2Febdd5b65-b5cb-4d4b-bd78-92ff8cc2f243,ReVISit 2: A Full Experiment Life Cycle User Study Framework,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Self-Supervised Continuous Colormap Recovery from a 2D Scalar Field Visualizations without a Legend,EARLY_ACCESS/c1893c22-b83f-4f15-82d7-c8f37bd65213,"Recovering a continuous colormap from a single 2D scalar field visualization can be quite challenging, especially in the absence of a corresponding color legend. In this paper, we propose a novel colormap recovery approach that extracts the colormap from a color-encoded 2D scalar field visualization by simultaneously predicting the colormap and underlying data using a decoupling-and-reconstruction strategy. Our approach first separates the input visualization into colormap and data using a decoupling module, then reconstructs the visualization with a differentiable color-mapping module. To guide this process, we design a reconstruction loss between the input and reconstructed visualizations, which serves both as a constraint to ensure strong correlation between colormap and data during training, and as a self-supervised optimizer for fine-tuning the predicted colormap of unseen visualizations during inferencing. To ensure smoothness and correct color ordering in the extracted colormap, we introduce a compact colormap representation using cubic B-spline curves and an associated color order loss. We evaluate our method quantitatively and qualitatively on a synthetic dataset and a collection of real-world visualizations from the VIS30K dataset~\cite{Chen21}. Additionally, we demonstrate its utility in two prototype applications-colormap adjustment and colormap transfer-and explore its generalization  to  visualizations with color legends and ones encoded using discrete color palettes.",Hongxu Liu;Xinyu Chen;Haoyang Zheng;Manyi Li;Zhenfan Liu;Fumeng Yang;Yunhai Wang;Changhe Tu;Qiong Zeng,,True,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2Fc1893c22-b83f-4f15-82d7-c8f37bd65213,Self-Supervised Continuous Colormap Recovery from a 2D Scalar Field Visualizations without a Legend,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Set Size Matters: Capacity-Limited Perception of Grouped Spatial-Frequency Glyphs,EARLY_ACCESS/1acd711b-69a4-486f-9955-0f6adb58292b,"Recent work suggests that shape can encode quantitative data via a mapping between value and spatial frequency (SF). However, the set-size effect when perceiving multiple SF based items remains unclear. While automatic feature extraction has been found to be less affected by set size (number of items in a group), higher-level processes for making perceptual decisions tend to require increased cognitive demand. To investigate the set-size effect on comparing integrated SF based items, we used a risk-based scenario to assess discrimination performance. Participants were asked to discriminate between pairs of maps containing multiple SF glyphs, in which each glyph represents one of four discrete levels (none, low, medium, high), forming an aggregate “risk strength” per map. The set size was also adjusted across conditions, ranging from small (3 items) to large (7 items). Discrimination sensitivity is modeled with a logistic function and response time with a mixed-effect linear model. Results show that smaller set sizes and lower overall strength enable more precise discrimination, with faster response times for larger differences between maps. Incorporating set size and overall strength into the logistic model, we found that these variables both independently and jointly influence discrimination sensitivity. We suggest these results point towards capacity-limited processes rather than purely automatic ensemble coding. Our findings highlight the importance of set size and overall signal strength when presenting multiple SF glyphs in data visualization.",Yiran Li;Shan Shao;Peter Baudains;Andrew Meso;Nick Holliman;Alfie Abdul-Rahman;Rita Borgo,,True,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F1acd711b-69a4-486f-9955-0f6adb58292b,Set Size Matters: Capacity-Limited Perception of Grouped Spatial-Frequency Glyphs,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,TFZ: Topology-Preserving Compression of 2D Symmetric and Asymmetric Second-Order Tensor Fields,EARLY_ACCESS/84ab0ade-9529-4942-b5e1-ca953e56c880,"In this paper, we present a novel compression framework, TFZ, that preserves the topology of 2D symmetric and asymmetric second-order tensor fields defined on flat triangular meshes. A tensor field assigns a tensor—a multi-dimensional array of numbers — to each point in space. Tensor fields, such as the stress and strain tensors, and the Riemann curvature tensor, are essential to both science and engineering. The topology of tensor fields captures the core structure of data, and is useful in various disciplines, such as graphics (for manipulating shapes and textures) and neuroscience (for analyzing brain structures from diffusion MRI). Lossy data compression may distort the topology of tensor fields, thus hindering downstream analysis and visualization tasks. TFZ ensures that certain topological features are preserved during lossy compression. Specifically, TFZ preserves degenerate points essential to the topology of symmetric tensor fields and retains eigenvector and eigenvalue graphs that represent the topology of asymmetric tensor fields. TFZ scans through each cell, preserving the local topology of each cell, and thereby ensuring certain global topological guarantees. We showcase the effectiveness of our framework in enhancing the lossy scientific data compressors SZ3 and SPERR.",Nathaniel Gorski;Xin Liang;Hanqi Guo;Bei Wang,,,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2F84ab0ade-9529-4942-b5e1-ca953e56c880,TFZ: Topology-Preserving Compression of 2D Symmetric and Asymmetric Second-Order Tensor Fields,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,The Impact of Visual Segmentation on Lexical Word Recognition,EARLY_ACCESS/bf10121c-1a25-4391-b766-450e53b2348e,"When a reader encounters a word in English, they split the word into smaller orthographic units in the process of recognizing
its meaning. For example, “rough”, when split according to phonemes, is decomposed as r-ou-gh (not as r-o-ugh or r-ough), where
each group of letters corresponds to a sound. Since there are many ways to segment a group of letters, this constitutes a computational
operation that has to be solved by the reading brain, many times per minute, in order to achieve the recognition of words in text
necessary for reading. In English, the irregular relationships between groups of letters and sounds, and the wide variety of possible
groupings make this operation harder than in more regular languages such as Italian. If this segmentation takes a significant amount of
time in the process of recognizing a word, it is conceivable that providing segmentation information in the text itself could help the
reading process by reducing its computational cost. In this paper we explore whether and how different visual interventions from the
visualization literature could communicate segmentation information for reading and word recognition. We ran a series of pre-registered
lexical decision experiments with 192 participants that tested five main types of visual segmentations: outlines, spacing, connections,
underlines and color. The evidence indicates that, even with a moderate amount of training, these visual interventions always slow
down word identification, but each to a different extent (between 32.7ms—color technique—and 70.7ms—connection technique).
These findings are important because they indicate that, at least for typical adult readers with a moderate amount of specific training in
these visual interventions, accelerating the lexical decision task is unlikely. Importantly, the results also offer an empirical measurement
of the cost of a common set of visual manipulations of text, which can be useful for practitioners seeking to visualize alongside or within
text without impacting reading performance. Finally, the interaction between typographically encoded information and visual variables
presented unique patterns that deviate from existing theories, suggesting new directions for future inquiry.",Matthew Termuende;Kevin Larson;Miguel Nacenta,,True,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2Fbf10121c-1a25-4391-b766-450e53b2348e,The Impact of Visual Segmentation on Lexical Word Recognition,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval,EARLY_ACCESS/88a99aaa-b89c-494c-b810-2113f43914ab,"The dissemination of visualizations is primarily in the form of raster images, which often results in the loss of critical information such as source code, interactive features, and metadata. While previous methods have proposed embedding metadata into images to facilitate Visualization Image Data Retrieval (VIDR), most existing methods lack practicability since they are fragile to common image tampering during online distribution such as cropping and editing. To address this issue, we propose VisGuard, a tamper-resistant VIDR framework that reliably embeds metadata link into visualization images. The embedded data link remains recoverable even after substantial tampering upon images. We propose several techniques to enhance robustness, including repetitive data tiling, invertible information broadcasting, and an anchor-based scheme for crop localization. VisGuard enables various applications, including interactive chart reconstruction, tampering detection, and copyright protection. We conduct comprehensive experiments on VisGuard's superior performance in data retrieval accuracy, embedding capacity, and security against tampering and steganalysis, demonstrating VisGuard's competence in facilitating and safeguarding visualization dissemination and information conveyance.",Huayuan Ye;Juntong Chen;Shenzhuo Zhang;Yipeng Zhang;Changbo Wang;Chenhui Li,,,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F88a99aaa-b89c-494c-b810-2113f43914ab,VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,Visual Analytics Using Tensor Unified Linear Comparative Analysis,EARLY_ACCESS/b5fec96f-039e-4af2-93d6-898b9a2caece,"Comparing tensors and identifying their (dis)similar structures is fundamental in understanding the underlying phenomena for complex data. Tensor decomposition methods help analysts extract tensors’ essential characteristics and aid in visual analytics for tensors. In contrast to dimensionality reduction (DR) methods designed only for analyzing a matrix (i.e., second-order tensor), existing tensor decomposition methods do not support flexible comparative analysis. To address this analysis limitation, we introduce a new tensor decomposition method, named tensor unified linear comparative analysis (TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA integrates discriminant analysis and contrastive learning schemes for tensor decomposition, enabling flexible comparison of tensors. We also introduce an effective method to visualize a core tensor extracted from TULCA into a set of 2D visualizations. We integrate TULCA’s functionalities into a visual analytics interface to support analysts in interpreting and refining the TULCA results. We demonstrate the efficacy of TULCA and the visual analytics interface with computational evaluations and two case studies, including an analysis of log data collected from a supercomputer.",Naoki Okami;Kazuki Miyake;Naohisa Sakamoto;Jorji Nonaka;Takanori Fujiwara,,True,True,P;C;O,https://vispubs.com/?paper=EARLY_ACCESS%2Fb5fec96f-039e-4af2-93d6-898b9a2caece,Visual Analytics Using Tensor Unified Linear Comparative Analysis,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,"VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization",EARLY_ACCESS/9e05775a-056c-42f7-a3c2-e81281c5045a,"We present VizGenie, a self-improving, agentic framework that advances scientific visualization through large language model (LLM) by orchestrating of a collection of domain-specific and dynamically generated modules. Users initially access core functionalities—such as threshold-based filtering, slice extraction, and statistical analysis—through pre-existing tools. For tasks beyond this baseline, VizGenie autonomously employs LLMs to generate new visualization scripts (e.g., VTK Python code), expanding its capabilities on-demand. Each generated script undergoes automated backend validation and is seamlessly integrated upon successful testing, continuously enhancing the system’s adaptability and robustness. A distinctive feature of VizGenie is its intuitive natural language interface, allowing users to issue high-level feature-based queries (e.g., “visualize the skull” or “highlight tissue boundaries”). The system leverages image-based analysis and visual question answering (VQA) via fine-tuned vision models to interpret these queries precisely, bridging domain expertise and technical implementation. Additionally, users can interactively query generated visualizations through VQA, facilitating deeper exploration. Reliability and reproducibility are further strengthened by Retrieval-Augmented Generation (RAG), providing context-driven responses while maintaining comprehensive provenance records. Evaluations on complex volumetric datasets demonstrate significant reductions in cognitive overhead for iterative visualization tasks. By integrating curated domain-specific tools with LLM-driven flexibility, VizGenie not only accelerates insight generation but also establishes a sustainable, continuously evolving visualization practice. The resulting platform dynamically learns from user interactions, consistently enhancing support for feature-centric exploration and reproducible research in scientific visualization.",Ayan Biswas;Terece Turton;Nishath Ranasinghe;Shawn Jones;Bradley Love;William Jones;Aric Hagberg;Han-Wei Shen;Nathan Debardeleben;Earl Lawrence,,,True,P;V;O,https://vispubs.com/?paper=EARLY_ACCESS%2F9e05775a-056c-42f7-a3c2-e81281c5045a,"VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization",,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,VolMoVis: Real-Time Volume Generation and Motion Visualization with Dynamic Tomographic Reconstruction,EARLY_ACCESS/f84ebbf0-98bb-4a65-9640-3161c7449f60,"We present VolMoVis, a method for dynamic tomographic reconstruction that supports real-time volume generation and volumetric motion visualization from 2D projections. Visualizing the motion of 3D anatomical structures, such as organs and tumors, is critical for computer-aided interventions. However, conventional 4D volumetric reconstruction methods typically produce a limited set of volumes at discrete phases, suffering from low temporal resolution. Moreover, it often requires extensive segmentation of 3D structures or regions for visualizing volumetric data, making it challenging to segment and visualize dynamic volumes in real-time. To address these challenges, VolMoVis framework employs a continuous implicit neural representation that decomposes the dynamic volumetric data into a static reference volume and a continuous deformation field. This decomposition, along with an efficient deformation network, enables our framework to achieve real-time volume generation and volumetric visualization of continuous anatomical motions. We evaluate VolMoVis on both 4D digital phantoms and real patient datasets, demonstrating its effectiveness for accurate anatomical reconstruction and motion tracking. Furthermore, we highlight its capabilities in real-time simultaneous volume generation and tumor segmentation for visualizing dynamic volumes and 4D tumor tracking, showcasing its potential in image-guided radiation therapy.",Gaofeng Deng;Arie Kaufman,,,True,O,https://vispubs.com/?paper=EARLY_ACCESS%2Ff84ebbf0-98bb-4a65-9640-3161c7449f60,VolMoVis: Real-Time Volume Generation and Motion Visualization with Dynamic Tomographic Reconstruction,,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,"Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances",EARLY_ACCESS/4cf2c4a7-98c7-4d49-a5c2-410ecde134cb,"A growing body of work on visualization affordances highlights how specific design choices shape reader takeaways from information visualizations. However, mapping the relationship between design choices and reader conclusions often requires labor-intensive crowdsourced studies, generating large corpora of free-response text for analysis. To address this challenge, we explored alternative scalable research methodologies to assess chart affordances. We test four elicitation methods from human-subject studies: free response, visualization ranking, conclusion ranking, and salience rating, and compare their effectiveness in eliciting reader interpretations of line charts, dot plots, and heatmaps. Overall, we find that while no method fully replicates affordances observed in free-response conclusions, combinations of ranking and rating methods can serve as an effective proxy at a broad scale. The two ranking methodologies were influenced by participant bias towards certain chart types and the comparison of suggested conclusions. Rating conclusion salience could not capture the specific variations between chart types observed in the other methods. To supplement this work, we present a case study with GPT-4o, exploring the use of large language models (LLMs) to elicit human-like chart interpretations. This aligns with recent academic interest in leveraging LLMs as proxies for human participants to improve data collection and analysis efficiency. GPT-4o performed best as a human proxy for the salience rating methodology but suffered from severe constraints in other areas. Overall, the discrepancies in affordances we found between various elicitation methodologies, including GPT-4o, highlight the importance of intentionally selecting and combining methods and evaluating trade-offs.",Chase Stokes;Kylie Lin;Cindy Xiong Bearfield,,True,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F4cf2c4a7-98c7-4d49-a5c2-410ecde134cb,"Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances",,,,,,False,False,False,1.0,[],0,False,0.0,1,False
Vis,2025,"Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models",EARLY_ACCESS/1b3b1b15-e378-4c65-8984-717a7f7baa3a,"Systems relying on ML have become ubiquitous, but so has biased behavior within them. Research shows that bias significantly affects stakeholders’ trust in systems and how they use them. Further, stakeholders of different backgrounds view and trust the same systems differently. Thus, how ML models’ behavior is explained plays a key role in comprehension and trust. We survey explainability visualizations, creating a taxonomy of design characteristics. We conduct user studies to evaluate five state-of-the-art visualization tools (LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how taxonomy characteristics affect comprehension, bias perception, and trust for non-expert ML users. Surprisingly, we find an inverse relationship between comprehension and trust: the better users understand the models, the less they trust them. We investigate the cause and find that this relationship is strongly mediated by bias perception: more comprehensible visualizations increase people’s perception of bias, and increased bias perception reduces trust. We confirm this relationship is causal: Manipulating explainability visualizations to control comprehension, bias perception, and trust, we show that visualization design can significantly (p < 0.001) increase comprehension, increase perceived bias, and reduce trust. Conversely, reducing perceived model bias, either by improving model fairness or by adjusting visualization design, significantly increases trust even when comprehension remains high. Our work advances understanding of how comprehension affects trust and systematically investigates visualization’s role in facilitating responsible ML applications.",Zhanna Kaufman;Madeline Endres;Cindy Xiong Bearfield;Yuriy Brun,,True,True,P;O,https://vispubs.com/?paper=EARLY_ACCESS%2F1b3b1b15-e378-4c65-8984-717a7f7baa3a,"Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models",,,,,,False,False,False,1.0,[],0,False,0.0,1,False
